{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch_wrapper_own.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUr+ZIFhdQv5RxmNH8SZxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandSinhaProjects/PyTorch_Made_Easy_Wrapper/blob/main/model/torch_wrapper_own_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ppw5Gcyva7s"
      },
      "source": [
        "This is to make a **pytorch wrapper** to make it easy for us to use the training wrapper. \n",
        "\n",
        "What are the things that we might want to include ->\n",
        "\n",
        "- training loop\n",
        "- optimizer\n",
        "- scheduler\n",
        "- model\n",
        "\n",
        "Here we are making a wrapper that only needs two things to work which are ->\n",
        "\n",
        "- Dataset class\n",
        "- Model class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxocTkhFvT1v"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd-prtv6ygAJ"
      },
      "source": [
        "##This is a class inheritance for bringing the module.\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init(*args, **kwargs)\n",
        "    self.optimizer = None\n",
        "    self.scheduler = None\n",
        "    self.train_loader = None\n",
        "    self.valid_loader = None\n",
        "  \n",
        "  def forward(self, *args, **kwargs):\n",
        "    super().forward(*args, **kwargs)\n",
        "\n",
        "  def fetch_optimizer(self, *args, **kwargs):\n",
        "    return\n",
        "\n",
        "  def fetch_scheduler(self, *args, **kwargs):\n",
        "    return\n",
        "\n",
        "  def train_one_step(data, device):\n",
        "    self.optimizer.zero_grad()\n",
        "    for k,v in data.items():\n",
        "      data[k] = v.to(device)\n",
        "    _, loss = self(**data)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    self.scheduler.step()\n",
        "    return loss\n",
        "\n",
        "  def train_one_epoch(self, data_loader, device):\n",
        "    self.train()\n",
        "    epoch_loss = 0\n",
        "    for data in data_loader:\n",
        "      loss = self.train_one_step(data, device)\n",
        "      epoch_loss += loss\n",
        "    return epoch_loss/len(data_loader) #This for average loss\n",
        "\n",
        "  def fit(self, train_dataset, valid_dataset, batch_size, epochs, device):\n",
        "\n",
        "    if self.train_loader is None:\n",
        "      self.train_loader = torch.utils.data.Dataloader(\n",
        "          train_dataset,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True\n",
        "      )\n",
        "\n",
        "\n",
        "      self.valid_loader = torch.utils.data.Dataloader(\n",
        "          valid_dataset,\n",
        "          batch_size=batch_size,\n",
        "          shuffle=True\n",
        "      )\n",
        "    \n",
        "    self.optimizer = self.fetch_optimizer()\n",
        "\n",
        "    self.scheduler = self.fetch_scheduler()\n",
        "\n",
        "    if next(self.parameters()).device != device:\n",
        "      self.to(device)\n",
        "\n",
        "    for _ in range(epochs):\n",
        "      train_loss = self.train_one_epoch(self.train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc4dXz9o6s46"
      },
      "source": [
        "## This section of code is written to show how our wrapper works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvJFo5cu3anl"
      },
      "source": [
        "class MyModel(Model):\n",
        "  super().__init__()\n",
        "  def __init__(self, num_classes):\n",
        "    self.out = nn.Linear(128, numn_classes)\n",
        "  \n",
        "  def loss(self, outputs, targets):\n",
        "    if targets is None:\n",
        "      return None\n",
        "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "  def fetch_scheduler(self):\n",
        "    sch= torch.optim.lr_scheduler.StepLR(self.optimizers)\n",
        "    return sch\n",
        "\n",
        "  def fetch_optimizer(self):\n",
        "    #params = self.parameters\n",
        "    opt=torch.optim.Adam(self.parameters())\n",
        "\n",
        "  def forward(self, features, targets=None):\n",
        "    #Yet to define"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}